{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelsoumya/intro_to_LMMs/blob/main/02_open_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9rRgetBHHwb"
      },
      "source": [
        "# The OpenAI API\n",
        "\n",
        "In this section we will cover the basics of using the OpenAI API, including:\n",
        "- Chat Completions\n",
        "- Streaming\n",
        "- Vision input\n",
        "\n",
        "The beauty of the OpenAI API is that is very simple to use.\n",
        "\n",
        "In your environment you should have a file called `.env` with the following:\n",
        "\n",
        "```bash\n",
        "OPENAI_API_KEY=\"sk-proj-1234567890\"\n",
        "```\n",
        "\n",
        "We will give you this key in the workshop. __The key will be deactivated after the workshop!__\n",
        "\n",
        "You can then grab the key using python:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load files from your Google drive (if you are using Google Colab)"
      ],
      "metadata": {
        "id": "CICO8y_4M7_K"
      }
    },
    {
      "source": [
        "from google.colab import drive"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gMRC5eLZHeJ_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "source": [
        "drive.mount('/content/drive')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdTARTqmHg8t",
        "outputId": "84f487f2-4e34-4857-81bf-40b0b51578c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages"
      ],
      "metadata": {
        "id": "zb9tsV5rM_iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo1mSg5pHM_u",
        "outputId": "d802f148-092d-4699-a116-28949143fca8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (2.4.1)\n",
            "Collecting llama-index (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index-0.12.34-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (1.76.0)\n",
            "Collecting python-dotenv (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 5))\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 6)) (13.9.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 7)) (2.11.3)\n",
            "Collecting chromadb (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting streamlit (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10))\n",
            "  Downloading streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 11)) (4.51.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 12)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 13)) (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 14)) (1.6.1)\n",
            "Collecting pysyllables (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 15))\n",
            "  Downloading pysyllables-1.0.3-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 16)) (3.1.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 17)) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 18)) (2.0.2)\n",
            "Collecting PyMuPDF (from -r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 19))\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (12.5.82)\n",
            "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5,>=0.4.1 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13,>=0.12.34 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_core-0.12.34.post1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4,>=0.3.0 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5,>=0.4.0 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (3.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 6)) (2.19.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 7)) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 7)) (0.4.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (1.2.2.post1)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (1.32.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (3.10.16)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (2.32.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (0.10.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10))\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10))\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (6.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 11)) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 11)) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 11)) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 12)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 12)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 12)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 12)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 12)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 12)) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 14)) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 14)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 14)) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 16)) (3.0.2)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (1.36.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (3.11.15)\n",
            "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (1.2.18)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (2.0.40)\n",
            "Collecting tiktoken>=0.7.0 (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading https://download.pytorch.org/whl/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_cloud-0.1.19-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (4.13.4)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5,>=0.4.0->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_parse-0.6.20-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 6)) (0.1.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (25.2.10)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (0.53b1)\n",
            "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (2025.2)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (3.4.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (15.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (1.20.0)\n",
            "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (4.3.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (2.7)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 10)) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (3.21.0)\n",
            "Collecting llama-cloud-services>=0.6.20 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading llama_cloud_services-0.6.20-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3)) (3.2.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 8)) (0.6.1)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.34->llama-index->-r /content/drive/MyDrive/intro_to_LMMs/requirements-handson.txt (line 3))\n",
            "  Downloading https://download.pytorch.org/whl/colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading llama_index-0.12.34-py3-none-any.whl (7.0 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysyllables-1.0.3-py2.py3-none-any.whl (426 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.9/426.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.34.post1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_llms_openai-0.3.38-py3-none-any.whl (23 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_cloud-0.1.19-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.20-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_cloud_services-0.6.20-py3-none-any.whl (37 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=7b0facfe4daab564e32fb88a93fab1ec4e8d97e7b0a84100eaa9bdc5c8b41eb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: striprtf, pypika, filetype, durationpy, dirtyjson, watchdog, uvloop, uvicorn, python-dotenv, pysyllables, pypdf, PyMuPDF, overrides, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httptools, colorama, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, tiktoken, starlette, pydeck, posthog, opentelemetry-exporter-otlp-proto-common, griffe, coloredlogs, onnxruntime, llama-cloud, kubernetes, fastapi, dataclasses-json, banks, opentelemetry-instrumentation, llama-index-core, streamlit, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, opentelemetry-instrumentation-fastapi, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, chromadb, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed PyMuPDF-1.25.5 asgiref-3.8.1 backoff-2.2.1 banks-2.1.2 bcrypt-4.3.0 chroma-hnswlib-0.7.6 chromadb-1.0.7 colorama-0.4.6 coloredlogs-15.0.1 dataclasses-json-0.6.7 dirtyjson-1.0.8 durationpy-0.9 fastapi-0.115.9 filetype-1.2.0 griffe-1.7.3 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 llama-cloud-0.1.19 llama-cloud-services-0.6.20 llama-index-0.12.34 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.34.post1 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.11 llama-index-llms-openai-0.3.38 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.20 marshmallow-3.26.1 mmh3-5.1.0 mypy-extensions-1.1.0 onnxruntime-1.21.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-util-http-0.53b1 overrides-7.7.0 posthog-4.0.1 pydeck-0.9.1 pypdf-5.4.0 pypika-0.48.9 pysyllables-1.0.3 python-dotenv-1.1.0 starlette-0.45.3 streamlit-1.45.0 striprtf-0.0.26 tiktoken-0.9.0 typing-inspect-0.9.0 uvicorn-0.34.2 uvloop-0.21.0 watchdog-6.0.0 watchfiles-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "isMDM_spHHwc",
        "outputId": "057b48ca-5b4b-4b1a-8bff-9e2e6ad62734"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-23dacc82b11f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdotenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mOPENAI_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import dotenv\n",
        "import os\n",
        "from rich import print as rprint # for making fancy outputs\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Si2vSjDHHwd"
      },
      "source": [
        "## Chat Completions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFmOOp55HHwd"
      },
      "source": [
        "Calling a model is simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g2FR8IaHHwd",
        "outputId": "642be07c-d62d-487f-ddaf-f2ab6185bb3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Silent as the dusk,  \n",
            "Whiskers twitch in moonlit grace—  \n",
            "A samurai pounces.\n"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are Matsuo Basho, the great Japanese haiku poet.\"\n",
        "user_query = \"Can you give me a haiku about a Samurai cat.\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  #model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "  ],\n",
        "  max_tokens=128\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijo4di0bHHwd"
      },
      "source": [
        "Purrfect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnZqRJltHHwd"
      },
      "source": [
        "The API offers a number of _endpoints_ that allow you to interact with the models. The main one that we will cover here is the `/chat/completions` endpoint. This endpoint allows you to interact with the model in a conversational manner.\n",
        "\n",
        "Only 2 arguments are actually required for this endpoint:\n",
        "\n",
        "- `model: str` The model to use. For OpenAI, this includes:\n",
        "    - `'gpt-3.5-turbo'`\n",
        "\n",
        "    - `'gpt-4'`\n",
        "\n",
        "    - `'gpt-4o'`\n",
        "\n",
        "    - `'gpt-4o-mini'`\n",
        "    \n",
        "    - Any fine-tuned versions of these models.\n",
        "    \n",
        "    - Many specific versions of the above models.\n",
        "\n",
        "- `messages: list` A list of messages that the model should use to generate a response. Each entry in the list of messages comes in the form:\n",
        "\n",
        "```python\n",
        "{\"role\": \"<role>\", \"content\": \"<content>\", \"name\": \"<name>\"}\n",
        "```\n",
        "\n",
        "Where `<role>` can take one of the following forms:\n",
        "\n",
        "- `'system'` This is a system level prompt, designed to guide the conversation. For example:\n",
        "\n",
        "_\"You are a customer service bot.\"_\n",
        "\n",
        "- `'user'` This is direct input from the user. For example:\n",
        "\n",
        "_\"How do I reset my password?\"_\n",
        "\n",
        "- `'assistant'` This is the response from the model. For example:\n",
        "\n",
        "_\"To reset your password, please visit our website and click on the 'Forgot Password' link.\"_\n",
        "\n",
        "So all of this fed into one message list would look like this:\n",
        "\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a customer service bot.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I reset my password?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"To reset your password, please visit our website and click on the 'Forgot Password' link.\"}\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umjn8s9uHHwe"
      },
      "source": [
        "### Additional arguments\n",
        "The `/chat/completions` endpoint also accepts a number of additional arguments that can be used to alter the response. These include (arguments are listed with their default values if applicable):\n",
        "\n",
        "- `max_tokens: int` The maximum number of tokens to generate in the response. Important to stop the model from generating too much text and racking up a huge bill.\n",
        "\n",
        "- `n: int = 1` The number of completions to generate. This is useful when you want to generate multiple completions and select the best one. You'll be charged for the _**total**_ number of tokens generated across all completions, so be careful with setting this too high.\n",
        "\n",
        "- `temperature: float = 1.0` The temperature of the model, ranging from 0.0 to 2. Use low values for deterministic responses, and high values for more creative responses.\n",
        "\n",
        "- `top_p: float = 1.0` The probability of sampling from the top `p` tokens. This is useful for controlling the diversity of the responses. Setting this to a higher values means the model is more likely to sample from a wider range of tokens.\n",
        "\n",
        "- `logprobs: bool = False` Whether to return the log probabilities of the tokens generated. This is useful when you want to understand how the model is making decisions.\n",
        "\n",
        "- `logit_bias: dict` A dictionary of logit biases to apply to the tokens. This is useful when you want to guide the model towards generating certain types of responses.\n",
        "\n",
        "- `response_format: str` The format of the response. We will cover this later...\n",
        "\n",
        "- `stream: bool = False` Whether to stream the response back to the client. This is useful when you want to get the response in real-time. Nobody likes to sit and wait for a response. Seeing the text generated as and when it is ready is a much better user experience.\n",
        "\n",
        "For a full list of arguments, check out the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/chat/create)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-33jOPgKHHwe"
      },
      "source": [
        "### Available models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0mnXVRcHHwe"
      },
      "source": [
        "Here we have used model `gpt-4o-mini`, but there are a range of models available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Df7lQbfHHwe",
        "outputId": "5afc8091-a60d-41a2-9234-a3afd92bb7e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai')\n",
            "Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system')\n",
            "Model(id='dall-e-2', created=1698798177, object='model', owned_by='system')\n",
            "Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system')\n",
            "Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system')\n",
            "Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system')\n",
            "Model(id='dall-e-3', created=1698785189, object='model', owned_by='system')\n",
            "Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal')\n",
            "Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system')\n",
            "Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system')\n",
            "Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')\n",
            "Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system')\n",
            "Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system')\n",
            "Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system')\n",
            "Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system')\n",
            "Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system')\n",
            "Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal')\n",
            "Model(id='gpt-4o', created=1715367049, object='model', owned_by='system')\n",
            "Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal')\n",
            "Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system')\n",
            "Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system')\n",
            "Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system')\n",
            "Model(id='gpt-4', created=1687882411, object='model', owned_by='openai')\n",
            "Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai')\n",
            "Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system')\n",
            "Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system')\n",
            "Model(id='babbage-002', created=1692634615, object='model', owned_by='system')\n",
            "Model(id='davinci-002', created=1692634301, object='model', owned_by='system')\n",
            "Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system')\n",
            "Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system')\n",
            "Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system')\n"
          ]
        }
      ],
      "source": [
        "for model in client.models.list():\n",
        "    print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQgg3rIAHHwe"
      },
      "source": [
        "As of writing `gpt-4o-2024-08-06` is the current best offering. But we'll stick with `gpt-4o-mini`, because it is cheaper and still highly capable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfWcu1xpHHwe"
      },
      "source": [
        "### The response object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo02rUioHHwe"
      },
      "source": [
        "What is the `response` object?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD7SfP8mHHwe",
        "outputId": "6d05effa-8702-438f-9504-c305fab2aa25"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-A5yRAL35y156KgXeY6uUBbLNHD4qh'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Silent pawsteps glide,  \\nMoonlight dances on the blade—  \\nFeline honor blinds.  '</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "            <span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1725987324</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_483d39d857'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57</span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-A5yRAL35y156KgXeY6uUBbLNHD4qh'\u001b[0m,\n",
              "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
              "        \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
              "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
              "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
              "            \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "            \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "                \u001b[33mcontent\u001b[0m=\u001b[32m'Silent pawsteps glide,  \\nMoonlight dances on the blade—  \\nFeline honor blinds.  '\u001b[0m,\n",
              "                \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
              "            \u001b[1m)\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1725987324\u001b[0m,\n",
              "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
              "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
              "    \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_483d39d857'\u001b[0m,\n",
              "    \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m20\u001b[0m, \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m37\u001b[0m, \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m57\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rprint(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLKDtYvdHHwe"
      },
      "source": [
        "There is some useful stuff in here, apart from the `content` property, such as the token usage. You might notice some other things too, like `function_call` and `tool_calls`. These are specific to OpenAI models, and not every model supports function calling or tools, so we won't cover them. We can achieve many of the same effects without them anyway."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YZzUOVjHHwe"
      },
      "source": [
        "## Streaming a response\n",
        "\n",
        "Streaming a response is mainly for user experience. It allows the user to see the response as it comes in, rather than waiting for the whole response to come in. For many applications, this might not be necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7o7cfxBHHwf",
        "outputId": "50aaf0bd-a97a-48f9-b17e-a58bb8dab3d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Silent paws in dusk,  \n",
            "Moonlit blade gleams in the night—  \n",
            "Fierce heart, whiskers twitch."
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "  ],\n",
        "  max_tokens=128,\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFFrNdlLHHwf"
      },
      "source": [
        "All this really does is create a streaming object, which acts like a generator. We can then print the chunk as it comes in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsEyMqPMHHwf"
      },
      "source": [
        "## Vision input\n",
        "A huge draw of OpenAI models is the ability to input vision data. This is useful for a wide range of applications, including:\n",
        "- Image captioning\n",
        "- Object detection\n",
        "- Face recognition\n",
        "- Image generation\n",
        "\n",
        "Let's try an example of inputting an image. First we need to look at the image:\n",
        "\n",
        "![plot](./imgs/figure.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHVKoUw0HHwf"
      },
      "source": [
        "Here is the caption from this figure:\n",
        "\n",
        "<table><tr><td>\n",
        "\n",
        "**Fig. 2 Spatial and temporal self-similarity and correlation in switching activity.**\n",
        "\n",
        "_(A) Percolating devices produce complex patterns of switching events that are self-similar in nature. The top panel contains 2400 s of data, with the bottom panels showing segments of the data with 10, 100, and 1000 times greater temporal magnification and with 3, 9, and 27 times greater magnification on the vertical scale (units of G0 = 2e2/h, the quantum of conductance, are used for convenience). The activity patterns appear qualitatively similar on multiple different time scales. (B and E) The probability density function (PDF) for changes in total network conductance, P(ΔG), resulting from switching activity exhibits heavy-tailed probability distributions. (C and F) IEIs follow power law distributions, suggestive of correlations between events. (D and G) Further evidence of temporal correlation between events is given by the autocorrelation function (ACF) of the switching activity (red), which decays as a power law over several decades. When the IEI sequence is shuffled (gray), the correlations between events are destroyed, resulting in a significant increase in slope in the ACF. The data shown in (B) to (D) (sample I) were obtained with our standard (slow) sampling rate, and the data shown in (E) to (G) (sample II) were measured 1000 times faster (see Materials and Methods), providing further evidence for self-similarity._\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4fZFXyfHHwf"
      },
      "source": [
        "This figure is taken from _[Avalanches and criticality in self-organized nanoscale networks, Mallinson et al., 2019.](https://www.science.org/doi/10.1126/sciadv.aaw8438)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7PEY_JqHHwf"
      },
      "source": [
        "Now let's use the OpenAI vision model to generate a caption for this figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7-hZ7VaHHwf"
      },
      "outputs": [],
      "source": [
        "prompt = (\n",
        "    \"This figure is a caption from a paper entitled Avalanches and criticality in self-organized nanoscale networks. \"\n",
        "    \"Please provide a caption for this figure. \"\n",
        "    \"You should describe the figure, grouping the panels where appropriate. \"\n",
        "    \"Feel free to make any inferences you need to.\"\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBTXLPOIHHwf"
      },
      "source": [
        "The process of calling a vision model is a little more involved, but OpenAI have a [convenient tutorial](https://platform.openai.com/docs/guides/vision) on how to do this.\n",
        "\n",
        "Essentially we need to first convert the image to a base64 string. We can then pass this to the OpenAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF_cbcUSHHwf"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"imgs/figure.jpeg\"\n",
        "\n",
        "\n",
        "def get_image_caption(image_path, prompt):\n",
        "  # Getting the base64 string\n",
        "  base64_image = encode_image(image_path)\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
        "  }\n",
        "\n",
        "  payload = {\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": prompt\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"max_tokens\": 512\n",
        "  }\n",
        "\n",
        "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "  return response.json()['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEqESYEHHHwf",
        "outputId": "757be807-59de-4b5a-a355-365e04e8f5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Figure Caption:**\n",
            "\n",
            "**Figure X:** Analysis of avalanche dynamics and criticality in self-organized nanoscale networks. \n",
            "\n",
            "**(A)** Time series data showing the fluctuations in conductance, \\(\\Delta G(G_0)\\), over varying observation periods (100 s, 10 s, 1 s, 0.1 s). The four panels illustrate distinct behavior and amplitude of fluctuations as time scales decrease. \n",
            "\n",
            "**(B) and (E)** Power law distributions, \\(P(\\Delta G)\\), of the amplitude of conductance fluctuations are presented on logarithmic scales, revealing power-law exponents \\(\\Delta G \\approx -2.59\\) (B) and \\(\\Delta G \\approx -2.36\\) (E). \n",
            "\n",
            "**(C) and (F)** Temporal distributions, \\(P(t)\\), showing the frequency of events over time. The corresponding power law exponents are \\(t \\approx -1.39\\) (C) and \\(t \\approx -1.30\\) (F), indicating scale-invariant behavior.\n",
            "\n",
            "**(D) and (G)** Characteristic avalanche sizes \\(A(t)\\) as a function of time, indicating distinct scaling regimes with exponents \\(t \\approx -0.19\\) and \\(t \\approx -0.23\\) for the upper and lower panels, respectively. The gray data in (D) suggests a crossover behavior in larger time scales. \n",
            "\n",
            "These results highlight the critical dynamics and self-organized behavior of the nanoscale networks under study.\n"
          ]
        }
      ],
      "source": [
        "caption = get_image_caption(image_path, prompt)\n",
        "print(caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQKVkmN8HHwf"
      },
      "source": [
        "I mean, I don't know about you, but I think that's incredible. Let's consider what it has done:\n",
        "- Correctly grouped the panels in the same way the real caption did.\n",
        "- Provided information on the observation periods.\n",
        "- Drawn out the important information, such as critical exponents.\n",
        "- Made the link between power law distributions and scale-free behaviour.\n",
        "\n",
        "However, it has failed to provide information on temporal correlations, and it has not noticed the self-similarity in caption 1.\n",
        "\n",
        "But this is still quite impressive, and with more information we could potentially get some better captions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og7fkwiZHHwf"
      },
      "source": [
        "## Tools\n",
        "We can also give the model some tools to use - these are essentially just functions that we can call, and the role of the LLM is to generate the arguments to that function.\n",
        "\n",
        "Here is a simple example to do some maths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJOqzyorHHwf",
        "outputId": "dd5489de-f13a-4652-8db5-917703b7bcc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1382976"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are a helpful mathematician. You will only solve the problems given to you. Do not provide any additional information. Provide only the answer.\"\n",
        "user_query = \"What is 1056 * 1316?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_query},\n",
        "  ],\n",
        "  max_tokens=256,\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGc8iILgHHwf",
        "outputId": "326f0590-b830-44b8-d058-8e39c8c8cc74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1389696"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1056 * 1316"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W3s5H_lHHwf"
      },
      "source": [
        "So the LLM is not correct :(.\n",
        "\n",
        "To endow the model with \"tool use\", we add a function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL69T2H8HHwf"
      },
      "outputs": [],
      "source": [
        "def multiply(a: float, b: float) -> float:\n",
        "    return a * b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzdSgnc_HHwf"
      },
      "source": [
        "And then provide the model with a _schema_, which is just a description of the function in dictionary form (or JSON):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWftlKACHHwf"
      },
      "outputs": [],
      "source": [
        "tool_schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"multiply\",\n",
        "        \"description\": \"Given two floats, a and b, return the product of a and b.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"a\": {\n",
        "                    \"type\": \"number\",\n",
        "                    \"description\": \"The first number to multiply.\"\n",
        "                },\n",
        "                \"b\": {\n",
        "                    \"type\": \"number\",\n",
        "                    \"description\": \"The second number to multiply.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"a\", \"b\"],\n",
        "            \"additionalProperties\": False,\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "tools = [\n",
        "    tool_schema\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DotmYNKhHHwg"
      },
      "source": [
        "When we make function calls with an LLM, we have to let it know that it has access to one or more tools. We do this by passing in the `tools` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ-6BYPvHHwg",
        "outputId": "80c817b3-d195-49eb-ba81-fdc12b359013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletionMessageToolCall(id='call_1vXSH7jPzjHCLYBlccyzewuB', function=Function(arguments='{\"a\":1056,\"b\":1316}', name='multiply'), type='function')\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_query},\n",
        "    ],\n",
        "    max_tokens=256,\n",
        "    tools=tools,\n",
        "    )\n",
        "\n",
        "print(response.choices[0].message.tool_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1J4NxicHHwg"
      },
      "source": [
        "So now in our `response`, we have this extra part called `tool_calls` that we can extract information from - in this case the arguments to the `multiply` function.\n",
        "\n",
        "Note that you could achieve a similar result with appropriate prompting - e.g. \"Extract only the arguments to a function that multiplies two numbers.\"\n",
        "\n",
        "We unpack the actual arguments as a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xbfkzjoHHwg",
        "outputId": "c65871bf-3efd-477d-af4f-5b8381a2a6ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'a': 1056, 'b': 1316}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "tool_call = response.choices[0].message.tool_calls[0]\n",
        "arguments = json.loads(tool_call.function.arguments)\n",
        "print(arguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcCToI0jHHwj"
      },
      "source": [
        "And we now feed the arguments into our `multiply` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DL204YJHHwj",
        "outputId": "4d52e510-5440-4e6b-8391-cb0bebf037c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1389696\n"
          ]
        }
      ],
      "source": [
        "result = multiply(**arguments)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S1q_5iVHHwj"
      },
      "source": [
        "So now we have the answer. We can either just return this, or we can feed it back into the LLM. We need to provide the model with the `tool_calls[0].id`, so that it can associate response messages of the tool type with the correct tool call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEsUSd1vHHwj",
        "outputId": "2ec1a1fe-b941-4922-d384-e17989b3e671"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1389696'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_call_result = {\n",
        "    \"role\": \"tool\",\n",
        "    \"content\": json.dumps({\n",
        "        \"a\" : arguments[\"a\"],\n",
        "        \"b\" : arguments[\"b\"],\n",
        "        \"result\": result\n",
        "    }),\n",
        "    \"tool_call_id\": response.choices[0].message.tool_calls[0].id\n",
        "}\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_query},\n",
        "        response.choices[0].message,\n",
        "        tool_call_result\n",
        "    ],\n",
        "    max_tokens=56\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iodR7FL0HHwj"
      },
      "source": [
        "This is quite a lot of work to multiply two numbers, but of course the power comes when doing more complex tasks.\n",
        "\n",
        "And this brings to light an interesting contrast. People talk a lot about \"agents\" and \"tools\" and \"systems\", and when we interact with ChatGPT, we get a single coherent experience. Sometimes it is difficult to distinguish between what the LLM is doing, and what the software engineers have built around it in order to create this seemless experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c4JFvUJHHwj"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}